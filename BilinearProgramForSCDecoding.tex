% ##QuantumErrorCorrection ##Decoding ##SurfaceCode ##CorrelatedErrors 
\documentclass[a4paper, english]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{babel}
\usepackage[cm]{fullpage}
\usepackage{float}
\usepackage{graphicx}
\usepackage{helvet}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{nicefrac}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{placeins}
\usepackage{verbatim}
\usepackage{xcolor}
\definecolor{gr}{gray}{0.9}
\renewcommand{\familydefault}{\sfdefault}
\title{Quantum Error Correction}
\subtitle{Correlated Decoding as Bilinear Programming}
\author{Ben Criger}
\date{\today}
\input{Qcircuit.tex}
\input{bencommands.tex}
\newlength\figureheight
\newlength\figurewidth
\setlength\figureheight{7cm}
\setlength\figurewidth{12cm}

\providecommand{\cnot}{\textsc{cnot}}

\usetikzlibrary{decorations.pathreplacing, decorations.pathmorphing, shapes.geometric, calc}

\begin{document}
\maketitle
\section{Introduction}
The toric code is a popular 2D nearest-neighbour LDPC code whose decoding can be efficiently accomplished by minimum-weight perfect matching. 
Qubits are placed on the edges of a square tiling of a 2D torus, and are subjected to one of three errors, with labels $X$, $Y$ and $Z$:
\begin{figure}[!h]
\centering
\begin{tikzpicture}[scale=0.75]
\draw[step=2, line width = 2pt, loosely dotted, line cap=round, line join=round] (0,0) grid (10,10);
\draw[step=2, line width = 2pt] (0,0) grid (9.99,9.99);
\foreach \x/\y/\err in {5/2/$Z$, 7/2/$Z$, 3/6/$X$, 2/7/$X$, 7/8/$Y$}{
\fill[black] (\x,\y) circle(14pt);
\node[white] at (\x,\y){\err};
}
\end{tikzpicture}
\caption{5-by-5 toric code lattice with an error configuration.
Qubits are supported on each edge, of which there are 50.
Five of these qubits have been subject to an error, drawn from the set $\set{X,\,Y,\,Z}$. 
Edges with no marker are not subject to error. 
Dotted edges indicate `wrapping around' the boundary to create a torus.}
\end{figure}

the parity checks of this code respond to $X$ and $Z$ errors, indicating when an odd number of $X$ errors surround a tile, and an odd number of $Z$ errors surround a vertex:
\begin{figure}[!h]
\centering
\begin{tikzpicture}[scale=0.75]
\draw[step=2, line width = 2pt, loosely dotted, line cap=round, line join=round] (0,0) grid (10,10);
\draw[step=2, line width = 2pt] (0,0) grid (9.99,9.99);
\foreach \x/\y/\err in {5/2/$Z$, 7/2/$Z$, 3/6/$X$, 2/7/$X$, 7/8/$Y$}{
\fill[black] (\x,\y) circle(14pt);
\node[white] at (\x,\y){\err};
}
\foreach \x/\y/\synd in {4/2/$Z$, 8/2/$Z$, 3/5/$X$, 1/7/$X$}{
\node[fill=black] at (\x,\y){\textcolor{white}{\synd}};
}
\end{tikzpicture}
\caption{Syndromes corresponding to $X$ and $Z$ errors.}
\end{figure}
\FloatBarrier
There are a few things that we can notice about these syndromes. 
First, if a continuous chain of errors of the either $X$ or $Z$ type appears on the lattice, two syndromes will appear at the endpoints. 
If only $X$ and $Z$ errors occur, then, the assignment of an error to a given syndrome in the toric code can be reduced to minimum-weight perfect matching, since the probability of an error chain can be expressed as a function of the distance between the syndromes.

If $Y$ errors could be independently detected and decoded in the same fashion, we'd be done. 
The problem is that, due to Quantum Code Technicalities\textsuperscript{\textregistered}, a $Y$ error produces a pair of $X$ \emph{and} $Z$ syndromes:
\begin{figure}[!h]
\centering
\begin{tikzpicture}[scale=0.75]
\draw[step=2, line width = 2pt, loosely dotted, line cap=round, line join=round] (0,0) grid (10,10);
\draw[step=2, line width = 2pt] (0,0) grid (9.99,9.99);
\foreach \x/\y/\err in {5/2/$Z$, 7/2/$Z$, 3/6/$X$, 2/7/$X$, 7/8/$Y$}{
\fill[black] (\x,\y) circle(14pt);
\node[white] at (\x,\y){\err};
}
\foreach \x/\y/\synd in {4/2/$Z$, 8/2/$Z$, 3/5/$X$, 1/7/$X$, 7/9/$X$, 7/7/$X$, 6/8/$Z$, 8/8/$Z$}{
\node[fill=black] at (\x,\y){\textcolor{white}{\synd}};
}
\end{tikzpicture}
\caption{Syndromes corresponding to $X$, $Y$ and $Z$ errors.}
\end{figure}

Our decoding problem is made more difficult by this, though decent performance can still be obtained by assuming that $Y$ errors occur with probability $\sim p^2$ (this works because a $Y$ error is the result of an $X$ and a $Z$ error occurring on the same qubit, bu we don't need to get into it), and decoding the $X$ and $Z$ syndromes independently.
This leaves a gap in the decoder's performance, that we're hoping to fix.
 
Existing approaches for this problem \cite{Fowler, DelfosseTillich} rely on \emph{reweighting}, adjusting the edge weights in an $X$ syndrome graph based on where $Z$ errors have been inferred, or vice versa. 
These weights are derived from \emph{conditional} probabilities, treating $p(Y)=p\left(X\given Z \right)$ or $p\left(Z\given X \right)$.

I have beef with this:
\begin{itemize}
\item It multiplies runtime by a factor of at least two, since the matching problem must be solved for one graph before the result can be applied to the other. 
This factor may be larger if we iterate, using each matching to reweight the other graph until convergence.
\item Performance can, in principle, depend on which graph we match first. It may be the case that both orders ($XZ$ and $ZX$) have to be attempted in parallel to get a decent solution. 
Even then, we have no guarantee that $Y$ errors can be accurately inferred given a decoding procedure that puts $X$ and $Z$ steps in order like this. 
\end{itemize}
We'd prefer to have a solution that can handle $Y$ errors `out of the box', to try to put them on the same footing as $X$ and $Z$. 
In the rest of this document, I show how arbitrary-length $Y$-chains can be accurately weighed, provided that the problem is generalised to a bilinear program. 
\section{Weights}
To derive the bilinear program, let's first show how the linear program for decoding syndromes independently is derived (see \cite{DKLP} for a longer and stronger explanation).
We begin by writing the probability of an $X$ error that produces pairs of $X$ syndromes, according to an independent error model:
\begin{equation}
p(E_X) = \prod_{C_X \in E_X } p^{\abs{C_X}} (1-p)^{n-\abs{C_X}},
\end{equation}
where $E_X$ is the $X$ error, $C_X$ is a continuous chain of $X$s that makes up part of the error, and $n$ is the number of qubits.
We can simplify this by taking the log: 
\begin{flalign}
p(E_X) &= (1-p)^n \prod_{C_X \in E_X } \left(\frac{p}{1-p} \right)^{\abs{C_X}}\\
& \propto \sum_{C_X \in E_X} \log \left(\frac{p}{1-p} \right)\abs{C_X} \propto -\sum_{C_X \in E_X} \abs{C_X} 
\end{flalign}
Maximizing the likelihood, then, is equivalent to minimizing the sum of lengths of the chains $C_X$ whose endpoints are given by the $X$ (or equivalently $Z$) syndromes. 

We can immediately see the problem. 
A single $Y$ error has an effective weight of two, since it is detected as a length-one $X$ chain and a length-one $Z$ chain in the absence of other errors. 
To fix this, let's consider a few examples. 
\section{Optimisation Problem}
\bibliographystyle{plain}
\bibliography{BilinearProgramForSCDecoding}
\section*{Appendices}
\appendix
\section{Measuring a Different Code Every Round}
If a bilinear program-based decoder doesn't work for whatever reason, do we have the option to measure a different set of stabilisers at every round that commute with the previous round's (e.g. CSS codes with $Y$ and $X$ strings instead of $X$ and $Z$) and reconstruct the error history from there?
A program of research suggests itself, where we go back to DKLP and figure out which errors form chains in 3D given the cycle of stabilisers that we measure. 
\end{document}